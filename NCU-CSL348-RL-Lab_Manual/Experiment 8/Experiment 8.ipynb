{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Statement:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a program to implement Q-Learning in Python."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required libraries\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial Q-table\n",
      "[[0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0.]]\n",
      "\n",
      "Reward Table\n",
      "[[ -1  -1  -1  -1   0  -1]\n",
      " [ -1  -1  -1   0  -1 100]\n",
      " [ -1  -1  -1   0  -1  -1]\n",
      " [ -1   0   0  -1   0  -1]\n",
      " [  0  -1  -1   0  -1 100]\n",
      " [ -1   0  -1  -1   0 100]]\n",
      "\n",
      "Episode 1 Path: [1, 3, 4, 5]\n",
      "State: 1, Action: 3, Reward: 0\n",
      "Q[1, 3]: 0.0\n",
      "State: 3, Action: 4, Reward: 0\n",
      "Q[3, 4]: 0.0\n",
      "State: 4, Action: 5, Reward: 100\n",
      "Q[4, 5]: 100.0\n",
      "Episode 1 Q-table:\n",
      "[[  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0. 100.]\n",
      " [  0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "Episode 2 Path: [0, 4, 5]\n",
      "State: 0, Action: 4, Reward: 0\n",
      "Q[0, 4]: 90.0\n",
      "State: 4, Action: 5, Reward: 100\n",
      "Q[4, 5]: 100.0\n",
      "Episode 2 Q-table:\n",
      "[[  0.   0.   0.   0.  90.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0. 100.]\n",
      " [  0.   0.   0.   0.   0.   0.]]\n",
      "\n",
      "Episode 3 Path: [4, 0, 4, 5]\n",
      "State: 4, Action: 0, Reward: 0\n",
      "Q[4, 0]: 81.0\n",
      "State: 0, Action: 4, Reward: 0\n",
      "Q[0, 4]: 90.0\n",
      "State: 4, Action: 5, Reward: 100\n",
      "Q[4, 5]: 100.0\n",
      "Episode 3 Q-table:\n",
      "[[  0.   0.   0.   0.  90.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]\n",
      " [  0.   0.   0.   0.   0.   0.]\n",
      " [ 81.   0.   0.   0.   0. 100.]\n",
      " [  0.   0.   0.   0.   0.   0.]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "states = 6  # S0, S1, S2, S3, S4, S5\n",
    "actions = 6  # The possible actions are to move to any of the 6 states\n",
    "gamma = 0.9  # Discount factor\n",
    "\n",
    "# Initializing the Q-table with Zeroes\n",
    "Q = np.zeros((states, actions))\n",
    "print(\"Initial Q-table\")\n",
    "print(Q)\n",
    "print()\n",
    "\n",
    "# Intializing the Reward Table\n",
    "R = np.full((states, actions), -1)  # Default reward is -1\n",
    "# Reward for Reaching Goal State is 100\n",
    "R[1, 5] = R[4, 5] = R[5, 5] = 100\n",
    "# Reward for Path Exising But Not Goal State is 0\n",
    "R[0, 4] = R[1, 3] = R[2, 3] = R[3, 1] = R[3, 2] = R[3,\n",
    "                                                    4] = R[4, 0] = R[4, 3] = R[5, 1] = R[5, 4] = 0\n",
    "\n",
    "print(\"Reward Table\")\n",
    "print(R)\n",
    "print()\n",
    "\n",
    "episodes_paths = [\n",
    "    [1, 3, 4, 5],\n",
    "    [0, 4, 5],\n",
    "    [4, 0, 4, 5]\n",
    "]\n",
    "\n",
    "# Function to update the Q-value\n",
    "\n",
    "\n",
    "def update_q(state, action, reward, next_state):\n",
    "    max_next_q = np.max(Q[next_state, :][Q[next_state, :] >= 0])\n",
    "    Q[state, action] = reward + gamma * max_next_q\n",
    "    print(f\"Q[{state}, {action}]: {Q[state, action]}\")\n",
    "\n",
    "\n",
    "# Q Learning Algorithm\n",
    "for episode, path in enumerate(episodes_paths):\n",
    "    print(f\"Episode {episode + 1} Path: {path}\")\n",
    "    for i in range(len(path)-1):\n",
    "        state = path[i]\n",
    "        next_state = path[i+1]\n",
    "        action = next_state\n",
    "        reward = R[state, action]\n",
    "        print(f\"State: {state}, Action: {action}, Reward: {reward}\")\n",
    "        update_q(state, action, reward, next_state)\n",
    "\n",
    "    # Printing the Q-table\n",
    "    print(f\"Episode {episode + 1} Q-table:\")\n",
    "    print(Q)\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
